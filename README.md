ğŸ’¼ Role-Based RAG Assistant
This is a Role-Based Retrieval-Augmented Generation (RAG) system built with LangChain, FastAPI, Streamlit, and Groq's LLMs. It classifies user queries, retrieves relevant document chunks based on user roles, and uses an LLM to synthesize answers strictly from those documents.

ğŸ“Œ Features
ğŸ” Role-Based Access: Ensures users can only view content relevant to their role (e.g., HR, Engineering, Finance).

ğŸ” Hybrid Semantic Search: Uses dense vector similarity + category-based filtering.

ğŸ§  Query Classification: Classifies questions into predefined categories like finance, hr, marketing, etc.

ğŸ¤– LLM Answering: Combines relevant document chunks and passes them to a Groq-hosted LLM to generate context-aware answers.

ğŸ–¥ï¸ Dual UI:

main.py exposes a FastAPI backend for API access.

app.py provides a clean, interactive Streamlit frontend.



ğŸ—‚ï¸ Folder Structure

```bash
.
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ classifier.py        # Query classification using LLM
â”‚   â”œâ”€â”€ search.py            # Role-filtered semantic search
â”‚   â””â”€â”€ embeddings.py        # Embeds CSV/MD documents
â”‚
â”œâ”€â”€ db/
â”‚   â””â”€â”€ clean_embeddings.pkl # Precomputed document embeddings
â”‚
â”œâ”€â”€ resources/               # Documents (Markdown & CSVs)
â”‚
â”œâ”€â”€ main.py                  # FastAPI backend
â”œâ”€â”€ app.py                   # Streamlit UI frontend
â”œâ”€â”€ .env                     # Env vars (GROQ_API_KEY)
â”œâ”€â”€ requirements.txt         # Required Python libraries
â””â”€â”€ README.md                # This file
```


ğŸš€ How to Run
1. Clone the Repository

```bash
git clone <your-repo-url>
cd <your-project-directory>
```

## 2. Install Dependencies

```bash
pip install -r requirements.txt
```

## 3. Set Up Environment Variables

Create a `.env` file in the project root:

```ini
GROQ_API_KEY=your_actual_groq_api_key_here
```

ğŸ’¡ Make sure you have access to Groqâ€™s API and the correct LLaMA-4 model is supported on your key.

## 4. Generate Embeddings

Before running the app, process the documents to generate vector embeddings:

```bash
python services/embeddings.py
```

This will create `db/clean_embeddings.pkl`.

## 5. Launch the Streamlit App (Frontend)

```bash
streamlit run app.py
```

## 6. Start the FastAPI Server (Backend)

```bash
uvicorn main:app --reload
```
